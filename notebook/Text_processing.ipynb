{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import re #library for regular expressions\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from string import punctuation \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path \n",
    "\n",
    "df = pd.read_csv ('dataframe/df_complete.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a new text column called text1 with the first pre-processing step: lower case\n",
    "df['text1'] = df['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "df['text1'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hyperlinks\n",
    "df['text1'] = [re.sub(r'https?:\\/\\/.\\S+', \"\", x) for x in df['text1']]\n",
    "\n",
    "# Remove websites and email address\n",
    "df['text1'] = [re.sub(r\"\\S+com\", \"\", x) for x in df['text1']]\n",
    "df['text1'] = [re.sub(r\"\\S+@\\S+\", \"\", x) for x in df['text1']]\n",
    "\n",
    "# Remove old style retweet text \"RT\"\n",
    "df['text1'] = [re.sub(r'^rt[\\s]+', '', x) for x in df['text1']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding Contractions\n",
    "# dictionary consisting of the contraction and the actual value\n",
    "#Questo forse si puo' togliere perchè le contrazioni poi vanno via eliminando le stopwords (vedi dopo)\n",
    "apos_dict = {\"'s\":\" is\",\"n't\":\" not\",\"'m\":\" am\",\"'ll\":\" will\",\n",
    "           \"'d\":\" would\",\"'ve\":\" have\",\"'re\":\" are\"}\n",
    "# replace the contractions\n",
    "for key,value in apos_dict.items():\n",
    "    if key in df['text1']:\n",
    "        df['text1'] = df['text1'].replace(key,value)\n",
    "\n",
    "# Remove punctuations (anche hashtag, @)\n",
    "df['text1'] =[re.sub(\"[\\W_]\", ' ', x) for x in df['text1']]\n",
    "df['text1']\n",
    "\n",
    "# Remove numbers\n",
    "df['text1'] =[re.sub(\"\\d+\", \"\", x) for x in df['text1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stopwords\n",
    "# The next step is to remove the useless words, namely, the stopwords. Stopwords are words that frequently appear in many articles,\n",
    "# but without significant meanings. Examples of stopwords are ‘I’, ‘the’, ‘a’, ‘of’.\n",
    "# spacy stopwords\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "print(len(stopwords)) # 326\n",
    "print(stopwords)\n",
    "\n",
    "# exclude the stopwords from the text\n",
    "df['text1'] = df['text1'].apply(lambda x: \" \".join(x for x in x.split() if x not in stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lemmatization\n",
    "#Another way of converting words to its original form is called stemming.\n",
    "#Lemmatization is taking a word into its original lemma, and stemming is taking the linguistic root of a word.\n",
    "\n",
    "# .lemma_ function from spacy \n",
    "\n",
    "def space(tweet):\n",
    "    doc = nlp(tweet)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "df['text1'] = df['text1'].apply(space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check which are the most common words\n",
    "# token dividen\n",
    "token_ = [i.split() for i in df[\"text1\"]]\n",
    "# token joined in one list \n",
    "#remove words with lenght < 3 and puntctuations !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "tokens = [item for sublist in token_ for item in sublist if len(item) > 3 and item not in punctuation]\n",
    "\n",
    "# Print most common word\n",
    "n_print = int(input(\"How many most common words to print: \"))\n",
    "print(\"\\nOK. The {} most common words are as follows\\n\".format(n_print))\n",
    "word_counter = Counter(tokens)\n",
    "for word, count in word_counter.most_common(n_print):\n",
    "    print(word, \": \", count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame of the most common words \n",
    "lst = word_counter.most_common(n_print)\n",
    "df_most_common = pd.DataFrame(lst, columns = ['Word', 'Count'])\n",
    "\n",
    "# Draw a bar chart dropping limerick \n",
    "plt.figure(figsize=(8,10))\n",
    "sns.barplot(y= 'Word', x = 'Count', data = df_most_common.drop([0,1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save de df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../dataframe/df_completec.csv') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cfed93451c29ea5b5eea9b91a43b4edb57dc235ce316cc9cede44e9cc75622d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
